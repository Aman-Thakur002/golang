/*
=============================================================================
                        ğŸš¦ GO RATE LIMITING TUTORIAL
=============================================================================

ğŸ“š CORE CONCEPT:
Rate limiting controls the rate at which operations are performed.
Go provides several mechanisms including time.Ticker, token bucket,
and sliding window algorithms for implementing rate limiting.

ğŸ”‘ KEY FEATURES:
â€¢ Control request/operation frequency
â€¢ Prevent system overload
â€¢ Implement backpressure
â€¢ Fair resource allocation

ğŸ’¡ REAL-WORLD ANALOGY:
Rate Limiting = Traffic Light System
- Ticker = Regular traffic light intervals
- Token bucket = Toll booth with limited tokens
- Sliding window = Traffic flow monitoring
- Backpressure = Traffic congestion management

ğŸ¯ WHY USE RATE LIMITING?
â€¢ Protect APIs from abuse
â€¢ Ensure fair resource usage
â€¢ Prevent system overload
â€¢ Comply with external API limits

=============================================================================
*/

package main

import (
	"context"
	"fmt"
	"sync"
	"time"
)

// ğŸ¯ BASIC RATE LIMITER WITH TICKER
func basicRateLimiter() {
	fmt.Println("ğŸ¯ Basic Rate Limiter (Ticker)")
	fmt.Println("==============================")

	// Allow 1 operation per second
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	requests := []string{"req1", "req2", "req3", "req4", "req5"}

	fmt.Println("Processing requests at 1 per second:")
	for _, req := range requests {
		<-ticker.C // Wait for ticker
		fmt.Printf("â° Processing %s at %s\n", req, time.Now().Format("15:04:05"))
	}
}

// ğŸª£ TOKEN BUCKET RATE LIMITER
type TokenBucket struct {
	tokens   chan struct{}
	ticker   *time.Ticker
	capacity int
	rate     time.Duration
	quit     chan bool
}

func NewTokenBucket(capacity int, rate time.Duration) *TokenBucket {
	tb := &TokenBucket{
		tokens:   make(chan struct{}, capacity),
		ticker:   time.NewTicker(rate),
		capacity: capacity,
		rate:     rate,
		quit:     make(chan bool),
	}

	// Fill bucket initially
	for i := 0; i < capacity; i++ {
		tb.tokens <- struct{}{}
	}

	// Start token refill goroutine
	go tb.refill()

	return tb
}

func (tb *TokenBucket) refill() {
	for {
		select {
		case <-tb.ticker.C:
			select {
			case tb.tokens <- struct{}{}:
				// Token added
			default:
				// Bucket full, skip
			}
		case <-tb.quit:
			return
		}
	}
}

func (tb *TokenBucket) Allow() bool {
	select {
	case <-tb.tokens:
		return true
	default:
		return false
	}
}

func (tb *TokenBucket) Wait() {
	<-tb.tokens
}

func (tb *TokenBucket) Stop() {
	tb.ticker.Stop()
	close(tb.quit)
}

// ğŸŒŠ SLIDING WINDOW RATE LIMITER
type SlidingWindow struct {
	mu       sync.Mutex
	requests []time.Time
	limit    int
	window   time.Duration
}

func NewSlidingWindow(limit int, window time.Duration) *SlidingWindow {
	return &SlidingWindow{
		requests: make([]time.Time, 0),
		limit:    limit,
		window:   window,
	}
}

func (sw *SlidingWindow) Allow() bool {
	sw.mu.Lock()
	defer sw.mu.Unlock()

	now := time.Now()
	cutoff := now.Add(-sw.window)

	// Remove old requests
	validRequests := sw.requests[:0]
	for _, req := range sw.requests {
		if req.After(cutoff) {
			validRequests = append(validRequests, req)
		}
	}
	sw.requests = validRequests

	// Check if we can allow this request
	if len(sw.requests) < sw.limit {
		sw.requests = append(sw.requests, now)
		return true
	}

	return false
}

func (sw *SlidingWindow) RequestCount() int {
	sw.mu.Lock()
	defer sw.mu.Unlock()
	return len(sw.requests)
}

// ğŸ›ï¸ ADAPTIVE RATE LIMITER
type AdaptiveRateLimiter struct {
	mu           sync.Mutex
	currentRate  time.Duration
	minRate      time.Duration
	maxRate      time.Duration
	successCount int
	errorCount   int
	lastAdjust   time.Time
	ticker       *time.Ticker
	tokens       chan struct{}
}

func NewAdaptiveRateLimiter(initialRate, minRate, maxRate time.Duration) *AdaptiveRateLimiter {
	arl := &AdaptiveRateLimiter{
		currentRate: initialRate,
		minRate:     minRate,
		maxRate:     maxRate,
		lastAdjust:  time.Now(),
		tokens:      make(chan struct{}, 1),
	}
	
	arl.tokens <- struct{}{} // Initial token
	arl.startTicker()
	
	return arl
}

func (arl *AdaptiveRateLimiter) startTicker() {
	if arl.ticker != nil {
		arl.ticker.Stop()
	}
	
	arl.ticker = time.NewTicker(arl.currentRate)
	go func() {
		for range arl.ticker.C {
			select {
			case arl.tokens <- struct{}{}:
			default:
			}
		}
	}()
}

func (arl *AdaptiveRateLimiter) Allow() bool {
	select {
	case <-arl.tokens:
		return true
	default:
		return false
	}
}

func (arl *AdaptiveRateLimiter) RecordSuccess() {
	arl.mu.Lock()
	defer arl.mu.Unlock()
	arl.successCount++
	arl.adjustRate()
}

func (arl *AdaptiveRateLimiter) RecordError() {
	arl.mu.Lock()
	defer arl.mu.Unlock()
	arl.errorCount++
	arl.adjustRate()
}

func (arl *AdaptiveRateLimiter) adjustRate() {
	now := time.Now()
	if now.Sub(arl.lastAdjust) < 5*time.Second {
		return // Don't adjust too frequently
	}

	total := arl.successCount + arl.errorCount
	if total < 10 {
		return // Need more data
	}

	errorRate := float64(arl.errorCount) / float64(total)
	
	if errorRate > 0.1 { // More than 10% errors, slow down
		newRate := time.Duration(float64(arl.currentRate) * 1.5)
		if newRate <= arl.maxRate {
			arl.currentRate = newRate
			arl.startTicker()
			fmt.Printf("ğŸŒ Slowing down to %v (error rate: %.1f%%)\n", arl.currentRate, errorRate*100)
		}
	} else if errorRate < 0.05 { // Less than 5% errors, speed up
		newRate := time.Duration(float64(arl.currentRate) * 0.8)
		if newRate >= arl.minRate {
			arl.currentRate = newRate
			arl.startTicker()
			fmt.Printf("ğŸš€ Speeding up to %v (error rate: %.1f%%)\n", arl.currentRate, errorRate*100)
		}
	}

	// Reset counters
	arl.successCount = 0
	arl.errorCount = 0
	arl.lastAdjust = now
}

// ğŸŒ HTTP RATE LIMITER EXAMPLE
type HTTPRateLimiter struct {
	limiters map[string]*TokenBucket
	mu       sync.RWMutex
	capacity int
	rate     time.Duration
}

func NewHTTPRateLimiter(capacity int, rate time.Duration) *HTTPRateLimiter {
	return &HTTPRateLimiter{
		limiters: make(map[string]*TokenBucket),
		capacity: capacity,
		rate:     rate,
	}
}

func (hrl *HTTPRateLimiter) Allow(clientID string) bool {
	hrl.mu.RLock()
	limiter, exists := hrl.limiters[clientID]
	hrl.mu.RUnlock()

	if !exists {
		hrl.mu.Lock()
		// Double-check after acquiring write lock
		if limiter, exists = hrl.limiters[clientID]; !exists {
			limiter = NewTokenBucket(hrl.capacity, hrl.rate)
			hrl.limiters[clientID] = limiter
		}
		hrl.mu.Unlock()
	}

	return limiter.Allow()
}

func main() {
	fmt.Println("ğŸš¦ RATE LIMITING TUTORIAL")
	fmt.Println("=========================")

	// ğŸ¯ DEMO 1: Basic Rate Limiter
	basicRateLimiter()

	// ğŸ¯ DEMO 2: Token Bucket Rate Limiter
	fmt.Println("\nğŸª£ Token Bucket Rate Limiter")
	fmt.Println("============================")

	bucket := NewTokenBucket(3, 500*time.Millisecond) // 3 tokens, refill every 500ms
	defer bucket.Stop()

	fmt.Println("Token bucket (capacity: 3, refill: 500ms):")
	
	// Burst of requests
	for i := 1; i <= 8; i++ {
		if bucket.Allow() {
			fmt.Printf("âœ… Request %d allowed at %s\n", i, time.Now().Format("15:04:05.000"))
		} else {
			fmt.Printf("âŒ Request %d denied at %s\n", i, time.Now().Format("15:04:05.000"))
		}
		time.Sleep(200 * time.Millisecond)
	}

	// ğŸ¯ DEMO 3: Sliding Window Rate Limiter
	fmt.Println("\nğŸŒŠ Sliding Window Rate Limiter")
	fmt.Println("==============================")

	window := NewSlidingWindow(5, 2*time.Second) // 5 requests per 2 seconds

	fmt.Println("Sliding window (5 requests per 2 seconds):")
	
	for i := 1; i <= 10; i++ {
		if window.Allow() {
			fmt.Printf("âœ… Request %d allowed (count: %d) at %s\n", 
				i, window.RequestCount(), time.Now().Format("15:04:05.000"))
		} else {
			fmt.Printf("âŒ Request %d denied (count: %d) at %s\n", 
				i, window.RequestCount(), time.Now().Format("15:04:05.000"))
		}
		time.Sleep(300 * time.Millisecond)
	}

	// ğŸ¯ DEMO 4: Adaptive Rate Limiter
	fmt.Println("\nğŸ›ï¸ Adaptive Rate Limiter")
	fmt.Println("========================")

	adaptive := NewAdaptiveRateLimiter(
		1*time.Second,   // initial rate
		200*time.Millisecond, // min rate (fastest)
		3*time.Second,   // max rate (slowest)
	)

	fmt.Println("Adaptive rate limiter (adjusts based on success/error rate):")
	
	// Simulate requests with varying success rates
	for i := 1; i <= 30; i++ {
		if adaptive.Allow() {
			// Simulate success/error
			if i < 10 || i > 20 {
				adaptive.RecordSuccess()
				fmt.Printf("âœ… Request %d succeeded\n", i)
			} else {
				adaptive.RecordError()
				fmt.Printf("âŒ Request %d failed\n", i)
			}
		} else {
			fmt.Printf("â³ Request %d rate limited\n", i)
		}
		time.Sleep(100 * time.Millisecond)
	}

	// ğŸ¯ DEMO 5: HTTP Rate Limiter (per client)
	fmt.Println("\nğŸŒ HTTP Rate Limiter (Per Client)")
	fmt.Println("=================================")

	httpLimiter := NewHTTPRateLimiter(2, 1*time.Second) // 2 requests, refill every second

	clients := []string{"client1", "client2", "client1", "client3", "client1", "client2"}
	
	fmt.Println("Per-client rate limiting:")
	for i, client := range clients {
		if httpLimiter.Allow(client) {
			fmt.Printf("âœ… Request %d from %s allowed\n", i+1, client)
		} else {
			fmt.Printf("âŒ Request %d from %s denied\n", i+1, client)
		}
		time.Sleep(200 * time.Millisecond)
	}

	// ğŸ¯ DEMO 6: Rate Limiting with Context
	fmt.Println("\nâ° Rate Limiting with Context")
	fmt.Println("=============================")

	ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second)
	defer cancel()

	rateLimitedWork := func(ctx context.Context, id int) error {
		ticker := time.NewTicker(500 * time.Millisecond)
		defer ticker.Stop()

		select {
		case <-ticker.C:
			fmt.Printf("ğŸ”„ Work %d completed\n", id)
			return nil
		case <-ctx.Done():
			fmt.Printf("â° Work %d cancelled: %v\n", id, ctx.Err())
			return ctx.Err()
		}
	}

	fmt.Println("Rate limited work with context timeout:")
	var wg sync.WaitGroup
	for i := 1; i <= 8; i++ {
		wg.Add(1)
		go func(id int) {
			defer wg.Done()
			rateLimitedWork(ctx, id)
		}(i)
	}
	wg.Wait()

	// ğŸ¯ DEMO 7: Backpressure Example
	fmt.Println("\nğŸ”™ Backpressure Example")
	fmt.Println("=======================")

	jobs := make(chan int, 3) // Small buffer to demonstrate backpressure
	results := make(chan int, 3)

	// Worker with rate limiting
	go func() {
		ticker := time.NewTicker(800 * time.Millisecond)
		defer ticker.Stop()

		for job := range jobs {
			<-ticker.C // Rate limit
			result := job * 2
			results <- result
			fmt.Printf("ğŸ”„ Processed job %d -> %d\n", job, result)
		}
		close(results)
	}()

	// Producer with backpressure handling
	go func() {
		defer close(jobs)
		for i := 1; i <= 8; i++ {
			select {
			case jobs <- i:
				fmt.Printf("ğŸ“¤ Sent job %d\n", i)
			default:
				fmt.Printf("ğŸš« Job %d blocked (backpressure)\n", i)
				// In real scenario, might retry later or drop
				time.Sleep(200 * time.Millisecond)
				jobs <- i // Retry
				fmt.Printf("ğŸ“¤ Sent job %d (retry)\n", i)
			}
		}
	}()

	// Collect results
	for result := range results {
		fmt.Printf("ğŸ“¥ Received result: %d\n", result)
	}

	fmt.Println("\nâœ¨ All rate limiting demos completed!")
}

/*
=============================================================================
                              ğŸ“ LEARNING NOTES
=============================================================================

ğŸš¦ RATE LIMITING ALGORITHMS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ // Token Bucket                                                         â”‚
â”‚ â€¢ Fixed capacity bucket with tokens                                     â”‚
â”‚ â€¢ Tokens refilled at constant rate                                      â”‚
â”‚ â€¢ Allows bursts up to bucket capacity                                   â”‚
â”‚ â€¢ Good for: API rate limiting, traffic shaping                          â”‚
â”‚                                                                         â”‚
â”‚ // Sliding Window                                                       â”‚
â”‚ â€¢ Track requests in time window                                         â”‚
â”‚ â€¢ More accurate than fixed window                                       â”‚
â”‚ â€¢ Higher memory usage                                                   â”‚
â”‚ â€¢ Good for: Precise rate limiting, analytics                            â”‚
â”‚                                                                         â”‚
â”‚ // Fixed Window                                                         â”‚
â”‚ â€¢ Count requests in fixed time periods                                  â”‚
â”‚ â€¢ Simple but can allow bursts at boundaries                             â”‚
â”‚ â€¢ Low memory usage                                                      â”‚
â”‚ â€¢ Good for: Simple rate limiting, logging                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸª£ TOKEN BUCKET IMPLEMENTATION:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ type TokenBucket struct {                                               â”‚
â”‚     tokens   chan struct{}  // Token storage                           â”‚
â”‚     ticker   *time.Ticker   // Token refill timer                      â”‚
â”‚     capacity int            // Maximum tokens                          â”‚
â”‚     rate     time.Duration  // Refill rate                             â”‚
â”‚ }                                                                       â”‚
â”‚                                                                         â”‚
â”‚ func (tb *TokenBucket) Allow() bool {                                   â”‚
â”‚     select {                                                            â”‚
â”‚     case <-tb.tokens:                                                   â”‚
â”‚         return true                                                     â”‚
â”‚     default:                                                            â”‚
â”‚         return false                                                    â”‚
â”‚     }                                                                   â”‚
â”‚ }                                                                       â”‚
â”‚                                                                         â”‚
â”‚ func (tb *TokenBucket) Wait() {                                         â”‚
â”‚     <-tb.tokens  // Block until token available                        â”‚
â”‚ }                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â° TIMING PATTERNS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ // Simple ticker rate limiting                                          â”‚
â”‚ ticker := time.NewTicker(1 * time.Second)                               â”‚
â”‚ defer ticker.Stop()                                                     â”‚
â”‚ for range ticker.C {                                                    â”‚
â”‚     // Process one item per second                                      â”‚
â”‚ }                                                                       â”‚
â”‚                                                                         â”‚
â”‚ // Rate limiting with timeout                                           â”‚
â”‚ ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) â”‚
â”‚ defer cancel()                                                          â”‚
â”‚                                                                         â”‚
â”‚ ticker := time.NewTicker(500 * time.Millisecond)                       â”‚
â”‚ defer ticker.Stop()                                                     â”‚
â”‚                                                                         â”‚
â”‚ for {                                                                   â”‚
â”‚     select {                                                            â”‚
â”‚     case <-ticker.C:                                                    â”‚
â”‚         // Process item                                                 â”‚
â”‚     case <-ctx.Done():                                                  â”‚
â”‚         return ctx.Err()                                                â”‚
â”‚     }                                                                   â”‚
â”‚ }                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸŒŠ BACKPRESSURE HANDLING:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ // Non-blocking send with backpressure                                  â”‚
â”‚ select {                                                                â”‚
â”‚ case jobQueue <- job:                                                   â”‚
â”‚     // Job sent successfully                                            â”‚
â”‚ default:                                                                â”‚
â”‚     // Queue full, handle backpressure                                  â”‚
â”‚     // Options: drop, retry later, block, return error                  â”‚
â”‚ }                                                                       â”‚
â”‚                                                                         â”‚
â”‚ // Buffered channel for burst handling                                  â”‚
â”‚ jobs := make(chan Job, bufferSize)                                      â”‚
â”‚                                                                         â”‚
â”‚ // Rate limited worker                                                  â”‚
â”‚ ticker := time.NewTicker(rate)                                          â”‚
â”‚ for {                                                                   â”‚
â”‚     select {                                                            â”‚
â”‚     case job := <-jobs:                                                 â”‚
â”‚         <-ticker.C  // Wait for rate limit                              â”‚
â”‚         processJob(job)                                                 â”‚
â”‚     }                                                                   â”‚
â”‚ }                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š MONITORING AND METRICS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ type RateLimiterMetrics struct {                                        â”‚
â”‚     AllowedRequests  int64                                              â”‚
â”‚     DeniedRequests   int64                                              â”‚
â”‚     CurrentTokens    int                                                â”‚
â”‚     QueueLength      int                                                â”‚
â”‚     AverageWaitTime  time.Duration                                      â”‚
â”‚ }                                                                       â”‚
â”‚                                                                         â”‚
â”‚ // Metrics collection                                                   â”‚
â”‚ func (rl *RateLimiter) GetMetrics() RateLimiterMetrics {                â”‚
â”‚     return RateLimiterMetrics{                                          â”‚
â”‚         AllowedRequests: atomic.LoadInt64(&rl.allowed),                 â”‚
â”‚         DeniedRequests:  atomic.LoadInt64(&rl.denied),                  â”‚
â”‚         CurrentTokens:   len(rl.tokens),                                â”‚
â”‚         QueueLength:     len(rl.queue),                                 â”‚
â”‚     }                                                                   â”‚
â”‚ }                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ BEST PRACTICES:
â€¢ Choose algorithm based on requirements (burst vs smooth)
â€¢ Monitor rate limiter performance and adjust parameters
â€¢ Implement graceful degradation when limits are hit
â€¢ Use context for timeout and cancellation
â€¢ Consider per-client vs global rate limiting
â€¢ Log rate limiting events for analysis
â€¢ Test rate limiters under load

ğŸš¨ COMMON MISTAKES:
âŒ Not handling backpressure properly
âŒ Using fixed window without considering burst behavior
âŒ Not monitoring rate limiter effectiveness
âŒ Blocking operations in rate-limited paths
âŒ Not considering memory usage of sliding windows
âŒ Forgetting to stop tickers (memory leaks)

âš¡ PERFORMANCE CONSIDERATIONS:
â€¢ Token bucket: O(1) for allow/deny decisions
â€¢ Sliding window: O(n) where n is requests in window
â€¢ Memory usage varies by algorithm
â€¢ Consider using atomic operations for counters
â€¢ Profile rate limiter overhead in hot paths

ğŸ¯ REAL-WORLD APPLICATIONS:
â€¢ API rate limiting (per user/IP)
â€¢ Database connection throttling
â€¢ External service call limiting
â€¢ Resource usage control
â€¢ Traffic shaping and QoS
â€¢ Batch processing rate control
â€¢ Circuit breaker patterns

ğŸ”§ ADVANCED PATTERNS:
â€¢ Hierarchical rate limiting (global + per-user)
â€¢ Adaptive rate limiting based on system load
â€¢ Distributed rate limiting with Redis
â€¢ Rate limiting with priority queues
â€¢ Exponential backoff with rate limiting

=============================================================================
*/